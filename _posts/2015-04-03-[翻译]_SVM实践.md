---
layout : default
title : SVM 实践技巧
---

上完了Andrew Ng的Machine Learning课程，自然想用机器学习的工具做一些实际的东西，比如预测股市、房价之类高大上的东西，之前看了些SVM的资料，实践的时候却发现有些困难，翻译了一篇Classification方面的实践指导。

原文[A Practical Guide to Support Vector Classification](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)

## 1. Introduction

SVM是一个强有力的分类工具。虽然SVM比神经网络好用一些，如果用户对它不熟悉，还是没法得出满意的结果。我们在这里给出了一个初学者指南。

注意，这个指南不是给SVM研究人员看的，也不能让你获得最好的训练精度。我们也不是为了解决复杂问题的。我们的目的是为了教SVM新手一个快速掌握SVM&训练出好结果的诀窍。

虽然用户不需要知道SVM的底层原理，我们还是简单介绍一下吧，一个训练任务需要把数据分成training set和testing set。训练集中的每一个数据包含了一个target value（标签）和若干个属性（特征值）。SVM的目标就是造出一个能根据属性值有效预测target value的模型。

假设训练集中的数据是这样的(Xi,yi)(Xi 是向量，y 属于 {1,-1}) i = 1,...,l .SVM就是求下列函数的最优解。(不贴函数了)

我们还需要知道的是 K(Xi,Xj)叫做核函数(kernel function)，它对结果影响很大。有下列四种基本的核函数

* 线性核函数 (linear) 
* 多项式 (polynomial)
* 径向基函数法 （radial basis function，RBF)
* sigmoid

其中核函数还有三个参数 γ,r 和d


### 1.1 Real-World Examples

一些真实的数据，[下载地址](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/data/)


### 1.2 Proposed Procedure


很多用户是这么解决问题的：

* 把采集到的数据转化成SVM的格式
* 随便挑一个kernel，参数也随便选
* 测试验证

我们提出这样的方法

* 将数据转化成SVM格式
* 对数据进行scaling(这个单词只可意会)
* 考虑使用RBF核函数
* 用交叉验证法挑选C 和 γ
* 使用C和γ来训练training set
* 测试验证

## 2. Data Preprocessing

### 2.1 Categorical Feature

SVM 需要每一个数据都表示成一个实数组成的向量。因此，如果有类别属性(enum)，我们首先把他们转换成实数。我们鼓励用m个实数表示m维的属性(只有一个为1，其他为0)。举例子，{red,green,blue}可以被表示成(0,0,1),(0,1,0)和(1,0,0).我们的经验是如果数字不是特别大，这个编码方式会很稳定。


### 2.2 Scaling

Scaling 是非常重要的步骤，[这篇文章](ftp://ftp.sas.com/pub/neural/FAQ.html)解释了为什么scaling对Neural Networks这么重要，它也适用于SVM. scaling的主要优势在于避免了某些数值跨度大的属性“影响”太大导致忽略了数值跨度小的属性。另一个优势在于避免了数值跨度大带来的计算困难。kernel的数据主要来自属性向量，超大的属性数据会导致线性、多项式的kernel数据计算出现问题。我们推荐线性地将属性压缩到[-1,+1]或者[0,1].

当然喽，我们要同时对训练数据和测试数据做同样的scaling。举例子，如果我们把第一个属性的范围从[-10,10]压缩到[-1,+1]。如果测试数据的范围是[-11,8]，那么我们就必须把它压缩到[-1.1,+0.8]。

## 3. Model Selection

虽然有4个常用kernel，我们必须选择一个先试试，然后再选择C和kernel的参数。

### 3.1 RBF Kernel

总体上讲，RBFkernel是首要的选择。这个kernel非线性地将数据映射到更高的维度，所以它可以处理属性和label之间非线性的情况。并且,线性的kernel只不过是RBF的一个特例,sinmoid kernel在特定情况下表现和RBF非常相似。

最后，RBF计算复杂度较小(后面不译了，全是公式)

也有些情况是不适合RBF的，当特征的数值很大，用线性kernel就好了。

### 3.2 Cross-validation and Grid-search

RBF有两个参数：C和γ，我们事先是不知道C和γ的值的，因此模型选择工作是必须要做的。我们的目标是确定一个比较合适的(C,γ)，我们的分类才会比较精准。注意到它不一定能获得很高的精确度。上面说过，正常的策略是将data分为两个部分，其中一个部分是未知的(需要预测的)，对未知部分的预测更精确反应了分类器的性能，一个提高性能的方式就是交叉验证。

在v-fold 交叉验证过程中，我先将训练数据分成v个大小相同的子集。连续地将每一个子集用其他子集作为训练集得出的分类器进行分类。因此，每一个数据都会被当做测试数据分类一遍，交叉验证的准确度就是被正确分类的数据的比例

交叉验证可以防止过度拟合的问题(我对过度拟合的理解就是在数据量有限+噪声很大的时候，我们SVM得出的C和γ未必是最优解)。

我们推荐一种网格搜索(grid-search)来寻找最合适的C和γ。对C和γ值的尝试使用幂增长的方式，比如(C=2^-5,2^-3,...,2^15,γ=2^-15,2^-13,...,2^3)

grid-search很直观不过看起来太简单了。实际上有很多高级方法，(不翻译了，只要知道grid search最适合新手就行了)

## 4. Discussion


